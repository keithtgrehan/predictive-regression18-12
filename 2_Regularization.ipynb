{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b712e1",
   "metadata": {},
   "source": [
    "# Regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974dbdee",
   "metadata": {},
   "source": [
    "When training and evaluating a model, you will often have to deal with the problem of over- or underfitting. A simple way to prevent overfitting of a polynomial model is to reduce the number and degrees of polynomials. The linear regression model implemented in Scikit-Learn offers no further hyperparameter to constrain the complexity of the model. Fortunately, Scikit-Learn offers two other linear models: `Ridge Regression` and `Lasso Regression`. Both models can be used to reduce model complexity and prevent overfitting which can occur with simple linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901f751",
   "metadata": {},
   "source": [
    "In this notebook you will learn:\n",
    "* why regularization is used\n",
    "* how to apply ridge regression\n",
    "* how to apply lasso regression\n",
    "* how ridge and lasso regularization constrain the parameters/coefficients of the model slightly differently\n",
    "* which hyperparameter for ridge and lasso regression can be tuned to influence model complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1ee4a06",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a1e381",
   "metadata": {},
   "source": [
    "We will use the **Wine Quality Dataset** to show how ridge and lasso regression constrain the model complexity. Our dataset has 11 variables to predict the wine *quality*. \n",
    "For the purpose of this exercise we will also expand the dataset by generating interaction features. This means we will use not only the original features but also all products of feature pairs. \n",
    "\n",
    "The variables are (based on physiochemical tests): \n",
    "\n",
    "1 - fixed acidity\n",
    "\n",
    "2 - volatile acidity\n",
    "\n",
    "3 - citric acid\n",
    "\n",
    "4 - residual sugar\n",
    "\n",
    "5 - chlorides\n",
    "\n",
    "6 - free sulfur dioxide\n",
    "\n",
    "7 - total sulfur dioxide\n",
    "\n",
    "8 - density\n",
    "\n",
    "9 - pH\n",
    "\n",
    "10 - sulphates\n",
    "\n",
    "11 - alcohol\n",
    "\n",
    "Output variable (based on sensory data):\n",
    "\n",
    "12 - quality (score between 0 and 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e97740e",
   "metadata": {},
   "source": [
    "## Imports and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e21d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# Define random seed\n",
    "RSEED = 0\n",
    "\n",
    "# define figure size\n",
    "plt.rcParams['figure.figsize'] = (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/winequality-white.csv',sep=\";\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our features are numeric, and there are no missing values. So, in order to understand how different regularization methods work, we can skip data cleaning for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features and target\n",
    "features = df.columns.tolist()\n",
    "features.remove('quality')\n",
    "X = df[features]\n",
    "y = df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation plot\n",
    "ax = sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "213cc998",
   "metadata": {},
   "source": [
    "We will create interaction terms (= feature combinations. For example X1*X2) of all the variables that exist in the dataset.\n",
    "So, if we have three features (X1, X2, X3) and we want to create all interaction features of degree 2 (combinations of two only!), then we get in total 6 features. The three starting features plus all combinations of 2 feature: (X1, X2, X3, X1\\*X2, X1\\*X3, X2\\*X3). \n",
    "If you want to understand all feature engineering possibilities of the `PolynomialFeatures` method, [checkout the documentation of scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate interaction features\n",
    "poly = PolynomialFeatures(interaction_only=True, include_bias=False, degree=2)\n",
    "X_poly = poly.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea536769",
   "metadata": {},
   "source": [
    "To calculate how many interaction terms only exist for our 11 features (n) and degree of 2 (k) (combinations of 2 features only), you can use this formula to calculate [combinations](https://en.wikipedia.org/wiki/Combination): \n",
    "\n",
    "${\\displaystyle {\\binom {n}{k}}={\\frac {n(n-1)\\dotsb (n-k+1)}{k(k-1)\\dotsb 1}}=  {\\frac {n!}{k!(n-k)!}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our case with 11 features, we end up with 66 features (11 features + 55 combinations)\n",
    "poly.n_output_features_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f113eae",
   "metadata": {},
   "source": [
    "## Train-test-split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8a07557",
   "metadata": {},
   "source": [
    "Before we train our models we will split the data in order to have a test dataset for the evaluation of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f960ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, random_state=RSEED, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the shape of our dataframes\n",
    "print(\"Training samples: \", X_train.shape[0],\"  Nr. of Features: \", X_train.shape[1])\n",
    "print(\"Training lables: \", y_train.shape[0])\n",
    "print( \"----\"*12)\n",
    "print(\"Test samples: \",X_test.shape[0],\"  Nr. of Features: \",X_test.shape[1])\n",
    "print(\"Test lables: \",y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab7961",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb20f4",
   "metadata": {},
   "source": [
    "We will start by training a linear regression model. \n",
    "We can use sklearns `LinearRegression` class to train a model which contains all of our available features.\n",
    "\n",
    "For each of our model we will evaluate the performance with looking at the RMSE (root mean squared error).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# initialize and train model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "\n",
    "# RMSE on train and test set\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train)), 3))\n",
    "print(\"Test RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lin)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model performs well on the training data but the RMSE on the test set is much worse. This discrepancy between the performance on the trainings and test set is a clear sign of overfitting. Since `LinearRegression` has no hyperparameters for constraining model complexity we will need to use another model. \n",
    "\n",
    "For comparison sake, we will also have a look at the value of the highest coefficient and how many coefficients are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeff_info(model):\n",
    "    coeff_used = np.sum(model.coef_!=0)\n",
    "    print('The model is using', coeff_used, 'out of 66 features.')\n",
    "    print( \"The highest coefficient has a value of:\", max(model.coef_.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(lin_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6def36f3",
   "metadata": {},
   "source": [
    "## Ridge Regression (l2 regularization)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa44e9f4",
   "metadata": {},
   "source": [
    "A commonly used alternative to the \"normal\" linear regression model is `Ridge` regression. It is also a linear model that uses basically the same formula that is used for ordinary least squares. However, our ridge regression model will also try to keep the magnitude of coefficients to be as small as possible. In other words, all entries of *w* should be close to zero. We can also say, each feature should have as little effect on the outcome as possible while still predicting well. \n",
    "\n",
    "If you have a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) you can see that this model has a hyperparameter called *alpha* which can be adjusted. Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization. \n",
    "\n",
    "Let's see how ridge regression with different alpha values performs on our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a818f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ef52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train model with (default value) alpha = 10\n",
    "ridge_10 = Ridge(alpha=10)\n",
    "ridge_10.fit(X_train, y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_ridge = ridge_10.predict(X_test)\n",
    "y_pred_train = ridge_10.predict(X_train)\n",
    "\n",
    "# RMSE of test set\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train)), 3))\n",
    "print(\"Test  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_ridge)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(ridge_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a416300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train model with alpha = 1\n",
    "ridge_1 = Ridge(alpha=1)\n",
    "ridge_1.fit(X_train, y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_ridge_1 = ridge_1.predict(X_test)\n",
    "y_pred_train_1 = ridge_1.predict(X_train)\n",
    "\n",
    "# RMSE of test set\n",
    "\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train_1)), 3))\n",
    "print(\"Test  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_ridge_1)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(ridge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train model with alpha = 0.1\n",
    "ridge_01 = Ridge(alpha=0.1)\n",
    "ridge_01.fit(X_train, y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_ridge_01 = ridge_01.predict(X_test)\n",
    "y_pred_train_01 = ridge_01.predict(X_train)\n",
    "\n",
    "# RMSE of test set\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train_01)), 3))\n",
    "print(\"Test  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_ridge_01)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(ridge_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89a307b",
   "metadata": {},
   "source": [
    "You can see that all three models perform clearly better compared to the linear regression model and the difference in the train and test score is considerably lower. Thus our models are not overfitting as much as the linear regression.  \n",
    "In our case the  value of alpha = 10 provides the best results. The optimum setting of alpha depends on the particular dataset we are using.\n",
    "\n",
    "\n",
    "We can also visualize the magnitude of our coefficients. \n",
    "On the x-axis the number of all our coefficients(we have 66 variables) are plotted as coefficient index (b1, b2, b3 etc.) and on the y-axis the values for these coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of different coefficients\n",
    "# linear regression without regularization\n",
    "plt.plot(lin_reg.coef_, 'o', alpha=0.5, label=\"LinearRegression\", color='green')\n",
    "# ridge regression with lowest alpha value = regularization term\n",
    "plt.plot(ridge_01.coef_, 'v', label=\"Ridge alpha=0.1\", color='orange')\n",
    "# ridge regression with middle alpha value = regularization term\n",
    "plt.plot(ridge_1.coef_, '*', label=\"Ridge alpha=1\", color='red')\n",
    "# ridge regression with highest alpha value = regularization term\n",
    "plt.plot(ridge_10.coef_, 's', alpha=0.5, label=\"Ridge alpha=10\", color='blue')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Coefficient index\", fontsize=16)\n",
    "plt.ylabel(\"Coefficient magnitude\", fontsize=16)\n",
    "plt.hlines(0, 0, len(lin_reg.coef_))\n",
    "plt.ylim(-12, 10)\n",
    "plt.legend(fontsize=13, ncol=2, loc=(0, 1.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot nicely shows how the higher the alpha values get the closer the coefficients are to zero. \n",
    "The green dots representing the coefficients from our linear regression model without any regularization are sometimes so large that they are nearly outside our plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "761dcfdc",
   "metadata": {},
   "source": [
    "## Lasso Regression (l1 regularization)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f841c8bf",
   "metadata": {},
   "source": [
    "An alternative to `Ridge` is `Lasso` regression. Similarly to ridge regression lasso restricts coefficients to be close to zero. It does so in a slightly different way so that when using lasso some coefficients become exactly zero. This means some features are entirely ignored by the model. It can be seen as an automatic feature selection which makes models often easier to interpret and can reveal the most important features.\n",
    "\n",
    "\n",
    "\n",
    "A look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso) shows us that lasso has also a hyperparameter called *alpha* which can be adjusted. Lowering alpha allows us to fit a more complex model. If alpha is set too low, however, we end up getting a similar result as with linear regression since no regularization will take place.  \n",
    "\n",
    "Let's try lasso regression with different alpha values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e0392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aadb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train model with (default value) alpha = 0.5\n",
    "lasso_05 = Lasso(alpha=0.5, max_iter=int(10e5))\n",
    "lasso_05.fit(X_train,y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_lasso = lasso_05.predict(X_test)\n",
    "y_pred_train_lasso = lasso_05.predict(X_train)\n",
    "\n",
    "# RMSE of test set\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train_lasso)), 3))\n",
    "print(\"Test  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lasso)), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(lasso_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train model with alpha 0.05\n",
    "\n",
    "lasso_005 = Lasso(alpha=0.05, max_iter=100000)\n",
    "lasso_005.fit(X_train,y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_lasso_005 = lasso_005.predict(X_test)\n",
    "y_pred_train_lasso_005 = lasso_005.predict(X_train)\n",
    "\n",
    "# RMSE of test set\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train_lasso_005)), 3))\n",
    "print(\"Test  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lasso_005)), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(lasso_005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a20184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train model with alpha 0.005\n",
    "lasso_0005 = Lasso(alpha=0.005, max_iter=100000)\n",
    "lasso_0005.fit(X_train,y_train)\n",
    "\n",
    "# predict on test-set\n",
    "y_pred_lasso_0005 = lasso_0005.predict(X_test)\n",
    "y_pred_train_lasso_0005 = lasso_0005.predict(X_train)\n",
    "\n",
    "# RMSE of test set\n",
    "print(\"Train RMSE:\", round(np.sqrt(mean_squared_error(y_train, y_pred_train_lasso_0005)), 3))\n",
    "print(\"Test  RMSE:\", round(np.sqrt(mean_squared_error(y_test, y_pred_lasso_0005)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_info(lasso_0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c5e738d",
   "metadata": {},
   "source": [
    "You can see that our lasso model with alpha = 0.5 performs rather bad and only uses 10 features. Lowering alpha to allow for a more complex model results in a better performance. The RMSE of our models with alpha = 0.05 and alpha = 0.005 are quite similar but there is still quite a difference in the number of features which are used. Since less features often makes it easier to interpret our model we will go for alpha = 0.05. \n",
    "\n",
    "We can also visualize the coefficients of our lasso models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of different coefficients\n",
    "plt.plot(lin_reg.coef_, 'o', alpha=0.3, label=\"Linear Regression\", color='green')\n",
    "# lasso regression with lowest alpha value = regularization term\n",
    "plt.plot(lasso_0005.coef_, 'v', alpha=0.6, label=\"Lasso alpha=0.005\", color='orange')\n",
    "# lasso regression with middle alpha value = regularization term\n",
    "plt.plot(lasso_005.coef_, 's', alpha=0.3, label=\"Lasso alpha=0.05\", color='blue')\n",
    "# lasso regression with highest alpha value = regularization term\n",
    "plt.plot(lasso_05.coef_, '*', alpha=0.3,label=\"Lasso alpha=0.5\", color='red')\n",
    "\n",
    "plt.xlabel(\"Coefficient index\", fontsize=16)\n",
    "plt.ylabel(\"Coefficient magnitude\", fontsize=16)\n",
    "plt.hlines(0, 0, len(lin_reg.coef_))\n",
    "plt.ylim(-15, 15)\n",
    "plt.legend(fontsize=13, ncol=2, loc=(0, 1.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really see the differences in coefficient values for Lasso Regularization, we will zoom in a bit more. Remember, that most of the coefficients for linear regression will not be in the plotted y-range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of different coefficients\n",
    "plt.plot(lin_reg.coef_, 'o', alpha=0.3, label=\"Linear Regression\", color='green')\n",
    "# lasso regression with lowest alpha value = regularization term\n",
    "plt.plot(lasso_0005.coef_, 'v', alpha=0.6, label=\"Lasso alpha=0.005\", color='orange')\n",
    "# lasso regression with middle alpha value = regularization term\n",
    "plt.plot(lasso_005.coef_, 's', alpha=0.3, label=\"Lasso alpha=0.05\", color='blue')\n",
    "# lasso regression with highest alpha value = regularization term\n",
    "plt.plot(lasso_05.coef_, '*', alpha=0.3,label=\"Lasso alpha=0.5\", color='red')\n",
    "\n",
    "plt.xlabel(\"Coefficient index\", fontsize=16)\n",
    "plt.ylabel(\"Coefficient magnitude\", fontsize=16)\n",
    "plt.hlines(0, 0, len(lin_reg.coef_))\n",
    "plt.ylim(-1, 1)\n",
    "plt.legend(fontsize=13, ncol=2, loc=(0, 1.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For alpha = 0.5 we can see that most of the coefficients are actually zero and the others are also quite small. Even if the plot for our ridge regressions looked similar, the coefficients of the ridge models were close but never exactly zero. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "045d0bf7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ff3e6cb",
   "metadata": {},
   "source": [
    "Now you saw two different ways to prevent your linear regression model from overfitting. In practice, ridge regression is usually the first choice. However, if you have a large amount of features and want to improve the interpretability of your model it makes sense to go for lasso regression since it will eliminate some of your features. \n",
    "\n",
    "But there is also a third model implemented in scikit-learn: the [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html?highlight=elasticnet#sklearn.linear_model.ElasticNet) regression. This model combines the penalties of Lasso and Ridge. Often this combination works best.\n",
    "\n",
    "Feel free to experiment with the `ElasticNet` regression model and compare the results to the other regularization methods. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2009a974",
   "metadata": {},
   "source": [
    "### Further Reading\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74541e97",
   "metadata": {},
   "source": [
    "The examples in this notebook can be found similarly in Andreas C. MÃ¼llers and Sarah Guidos book **Introduction to Machine Learning with Python**. If you need further explanations have a look at the chapter on supervised learning/linear regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "90a9a1c80a0f1464204b841018c5a48f5dd2f2bd503cc25f80788506f5955db0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
