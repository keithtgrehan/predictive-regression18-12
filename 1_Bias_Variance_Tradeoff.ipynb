{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Trade-Off\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias–variance tradeoff is a central problem in supervised learning. The error of a machine learning model consists of 3 parts:\n",
    "\n",
    "$MSE = variance + bias^2 + noise$ \n",
    "\n",
    "The noise is also called *irreducible error*, it cannot be reduced by applying different models.\n",
    "The variance and bias term is influenced by the flexibility of the chosen model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/9/9f/Bias_and_variance_contributing_to_total_error.svg'\n",
    "HTML(f'<img src=\"{url}\" width=\"600\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, our goal is to minimize the total error (for example the mean squared error). Therefore, we would like to minimize the bias and variance term of the error.\n",
    "\n",
    "Unfortunately, it is typically impossible to do both simultaneously. **High-variance learning methods** may be able to represent their training set well but are at risk of **overfitting** to noisy or unrepresentative training data. In contrast, algorithms with **high bias** typically produce simpler models that may fail to capture important regularities (i.e. **underfit**) in the data.\n",
    "\n",
    "This is called the **bias-variance tradeoff**. What it means is that it might be wise to use a biased estimator, so long as it reduces our variance, assuming our goal is to minimize squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Objectives\n",
    "\n",
    "In this notebook you will learn...\n",
    "\n",
    "* how to split you data into a train and a test set using `train_test_split`\n",
    "* how to create more flexible linear models by using polynomial features\n",
    "* how to use the root mean squared error to evaluate your model's performance\n",
    "* to define bias and variance\n",
    "* what the bias variance trade-off is and see its implications on a specific example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "# import to divide our data into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import to create polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# import of the linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import of our evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    root_mean_squared_error,  # available in scikit-learn >= 1.4\n",
    ")\n",
    "RSEED = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants and plotting parameters\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['figure.figsize'] = (8,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally we wouldn't know the true relationship between the independent features (X) and our dependent variable (y). But to get an idea about the bias-variance tradeoff it is helpful to know the true relationship between those variables.\n",
    "Here, we know this relationship because we define it in our data generating process.\n",
    "\n",
    "We will choose x in the range from -7 to 7. And the true relationship between x and y is:\n",
    "\n",
    "\n",
    "$y = 10x - 2 x^2 + 0.5 x^3 + \\epsilon$\n",
    "\n",
    "The $\\epsilon$ is introducing some random and unpredictable noise to our data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data \n",
    "np.random.seed(4)\n",
    "# generating 100 random x-values between -7 and 7\n",
    "x = np.random.uniform(-7,7,size=100)\n",
    "\n",
    "# generating corresponding y variable according to our defined true relationship\n",
    "# adding some random noise with np.random.normal with mu = 0 and std = 20\n",
    "y = 10 * x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(0, 20, 100)\n",
    "\n",
    "# transforming the data to include another axis since this is expected for modelling with scikit-learn\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Most often, one goal of your data science projects is to train a model to make predictions for new instances. But training a model alone does not guarantee you that it will perform well on unseen data. Before we can \"trust\" our model and put it into production, we need to evaluate its performance on data on which it has **not** been trained. \n",
    "\n",
    "The simplest way to do this is to split our original dataset into a training dataset and a test dataset. The training dataset, as the name suggests, is used to fit (train) our model. We can then evaluate this model against the test dataset, which contains only instances that our model has never seen. Luckily, we do not have to split our dataset manually. We can make use of scikit-learns [`train-test-split()` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=RSEED) #Careful here: this is an unusual choice for the test_size. See paragraph below for more explainations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of train and test set\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells above we called the `train_test_split()` function on our feature(s) and target. We have also defined what percentage of the entire data set should end up in the test set (`test_size`).\n",
    "\n",
    "\n",
    "Another important parameter is `random_state`: without setting a random state, scikit-learn will **randomly** shuffle your data when calling the function and assign instances to the test set. As a result, the scores of your model will change across multiple function calls. In order to reproduce the same results each time you call a notebook or script, you need to control the shuffling that is applied to the data. You can do this by setting the parameter `random_state`. \n",
    "\n",
    "\n",
    "    Exercise:\n",
    "\n",
    "    The documentation of scikit-learn is very valuable. Have a look at the scikit-learn page for train-test-split. What is the default value for the test_size? \n",
    "\n",
    "The default values of scikit-learn are always a good starting point and a benchmark. So why are we deviating so much from the norm here? \n",
    "\n",
    "There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
    "\n",
    "In our case here, we want to show what \"greater variance\" for the estimated parameter means, therefore our split is a bit unconventionally. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our data. It is always good to get a feeling of the train and test data, and if these are coming from the same distribution or varying a lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(-7,7,500)\n",
    "y_plot_true = 10 * x_plot - 2 * (x_plot ** 2) + 0.5 * (x_plot ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(X_train, y_train, s=10,c=\"#193251\",alpha=0.7, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",label= '\"True\" Relationship')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Generated Data\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true relationship is exactly our data generating process:\n",
    "\n",
    "\n",
    "$y = 10x - 2 x^2 + 0.5 x^3 + \\epsilon$\n",
    "\n",
    "\n",
    "The data varies from this line, by the random error we have added as well.\n",
    "\n",
    "Overall, the train data spans nearly the same range in feature x as the test data. Only in the very negative range (< -6) there are no training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try out different models on our generated data. By using different flexible models, we will also see the effect of bias and variance of these models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model: linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "After defining our features and target and splitting the data into train and test sets, we can start modeling. As a first step, we create an instance of `LinearRegression()` and fit it on the **training data** with `.fit()`. Then we make predictions for both **train** and **test** sets using `.predict()`.\n",
    "\n",
    "> Note: scikit-learn provides `root_mean_squared_error` (since v1.4) to compute RMSE directly. This is the recommended way in newer versions (e.g., v1.7.2) instead of using `mean_squared_error(..., squared=False)`.\n",
    "\n",
    "We will report **R²** and **RMSE** for both train and test to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Fit the model using our train data\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on both train and test\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "y_pred_test  = lin_reg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have trained a model and made predictions on our test set. Now it's time to evaluate the performance of the model. We will print the R-squared and the RMSE. Scikit-learn doesn't provide a function for calculating the RMSE directly, but it offers the possibility to set the parameter `squared = False` when calculating the mean squared error which then returns the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metric\n",
    "def calculate_metrics(y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \"\"\"Calculate and print out RMSE and R2 for train and test data\n",
    "\n",
    "    Args:\n",
    "        y_train (array): true values of y_train\n",
    "        y_pred_train (array): predicted values of model for y_train\n",
    "        y_test (array): true values of y_test\n",
    "        y_pred_test (array): predicted values of model for y_test\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Metrics on training data\")\n",
    "    rmse_train = root_mean_squared_error(y_train, y_pred_train)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    print(\"RMSE:\", round(rmse_train, 3))\n",
    "    print(\"R2:\", round(r2_train, 3))\n",
    "    print(\"---\" * 10)\n",
    "\n",
    "    print(\"Metrics on test data\")\n",
    "    rmse_test = root_mean_squared_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    print(\"RMSE:\", round(rmse_test, 3))\n",
    "    print(\"R2:\", round(r2_test, 3))\n",
    "    print(\"---\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression Model:\")\n",
    "print(\"---\" * 10)\n",
    "calculate_metrics(y_train, y_pred_train, y_test, y_pred_test)\n",
    "\n",
    "print('Standard deviation of y:', np.std(y))\n",
    "print('Mean of y:', np.mean(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Standard deviation of y:', np.std(y))\n",
    "print('Mean of y:', np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal RMSE would be 0, which means that all predictions matched the expected values exactly.\n",
    "Here, we see, that the RMSE on the training data is with 33 smaller thant the RMSE on the test data (44).\n",
    "This is typical, as we calculated our coefficients based on the training data, the model is prone to fit the training data better than the unknown test data.\n",
    "\n",
    "Let's look at our model graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_lin_reg = lin_reg.intercept_ + lin_reg.coef_[0]*x_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(X_train, y_train, s=10,c=\"#193251\",alpha=0.7, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",label= '\"True\" Relationship')\n",
    "plt.plot(x_plot,line_lin_reg,c=\"#FFBA08\",label= 'Linear model')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the straight line is unable to capture the patterns in the data. This is an example of **underfitting**. The linear model is not flexible enough to capture the true relationship of x and y. This is called a **biased** model. The model has strong pre-assumptions about the relationship between x and y: that there is a linear relationship.\n",
    "\n",
    "With only one feature, we are able to see how our model performs by looking an the feature-target plot. But if we have multiple feature, this graph is not available for us anymore. To still be able to evaluate the model performance, we can plot the true y values vs. the predicted values and plot our residuals. With these plots we check if the residuals are random normal distributed (ideal situation) or if we see some pattern (still improvement of model possible!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def error_analysis(y_test, y_pred_test):\n",
    "    \"\"\"Generated true vs. predicted values and residual scatter plot for models\n",
    "\n",
    "    Args:\n",
    "        y_test (array): true values for y_test\n",
    "        y_pred_test (array): predicted values of model for y_test\n",
    "    \"\"\"     \n",
    "    # Calculate residuals\n",
    "    residuals = y_test - y_pred_test\n",
    "    \n",
    "    # Plot real vs. predicted values \n",
    "    fig, ax = plt.subplots(1,2, figsize=(15, 5))\n",
    "    plt.subplots_adjust(right=1)\n",
    "    plt.suptitle('Error Analysis')\n",
    "    \n",
    "    ax[0].scatter(y_pred_test, y_test, color=\"#FF5A36\", alpha=0.7)\n",
    "    ax[0].plot([-400, 350], [-400, 350], color=\"#193251\")\n",
    "    ax[0].set_title(\"True vs. predicted values\", fontsize=16)\n",
    "    ax[0].set_xlabel(\"predicted values\")\n",
    "    ax[0].set_ylabel(\"true values\")\n",
    "    ax[0].set_xlim((y_pred_test.min()-10), (y_pred_test.max()+10))\n",
    "    ax[0].set_ylim((y_test.min()-40), (y_test.max()+40))\n",
    "    \n",
    "    ax[1].scatter(y_pred_test, residuals, color=\"#FF5A36\", alpha=0.7)\n",
    "    ax[1].plot([-400, 350], [0,0], color=\"#193251\")\n",
    "    ax[1].set_title(\"Residual Scatter Plot\", fontsize=16)\n",
    "    ax[1].set_xlabel(\"predicted values\")\n",
    "    ax[1].set_ylabel(\"residuals\")\n",
    "    ax[1].set_xlim((y_pred_test.min()-10), (y_pred_test.max()+10))\n",
    "    ax[1].set_ylim((residuals.min()-10), (residuals.max()+10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "\n",
    "Residual = Observed – Predicted\n",
    "\n",
    "…positive values for the residual (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct.\n",
    "\n",
    "Optimally the residuals,\n",
    "- (1) are pretty symmetrically distributed, tending to cluster towards the middle of the plot.\n",
    "- (2) they’re clustered around the lower single digits of the y-axis (e.g., 0.5 or 1.5, not 30 or 150).\n",
    "- (3) in general, there aren’t any clear patterns.\n",
    "\n",
    "Ok. We still see some patterns, that our model is not capturing. Fox x-values under -150, we always overestimate the y value, between -150 and circa 0, we always underestimate the y value. For x values above 0 the target value is again generally overestimated.\n",
    "\n",
    "This is again displaying the underfitting of our highly biased model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second model: linear regression with polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "To overcome underfitting, we need to increase the complexity of the model. We can do this by generating a higher order equation through adding powers of the original features as new features. \n",
    "The linear model, \n",
    "\n",
    "\n",
    "$\\hat{y} = b_0 + b_1x $\n",
    "\n",
    "\n",
    "can be transformed to\n",
    "\n",
    "$\\hat{y} = b_0 + b_1x + b_2x^2$\n",
    "\n",
    "\n",
    "This is still considered to be a linear model as the **coefficients/weights** associated with the features are still linear. \n",
    "\n",
    "**x²** is only a feature. \n",
    "\n",
    "However the curve that we are fitting is quadratic in nature. To convert the original features into their higher order terms we will use the `PolynomialFeature` class provided by scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will add the second degree polynomial features\n",
    "polynomial_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "x_poly_train = polynomial_features.fit_transform(X_train)\n",
    "x_poly_test = polynomial_features.transform(X_test)\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(x_poly_train, y_train)\n",
    "y_pred_test = model_poly.predict(x_poly_test)\n",
    "y_pred_train = model_poly.predict(x_poly_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polynomial Regression Model (degree 2):\")\n",
    "print(\"---\"*10)\n",
    "calculate_metrics(y_train,y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the RMSE of the training as well as of the test data could be reduced by adding quadratic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_poly_2_reg = model_poly.intercept_ + model_poly.coef_[0][0] * x_plot + model_poly.coef_[0][1] * x_plot**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(X_train, y_train, s=10,c=\"#193251\",alpha=0.7, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",alpha=0.7, label= '\"True\" Relationship')\n",
    "plt.plot(x_plot,line_lin_reg,c=\"#FFBA08\",alpha=0.5,label= 'Linear model')\n",
    "plt.plot(x_plot,line_poly_2_reg,c=\"#FFBA08\",marker = \"o\",ms=1, alpha=0.8,label= 'Degree 2 model')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a linear regression model on the transformed features gives the above plot. It is quite clear from the plot that the quadratic curve is able to fit the data better than the linear line. If we compare the `RMSE` and `R²-score` of the two models we can also see the improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metric   | RMSE  | R²  | \n",
    "|---|---|---|\n",
    "| **Linear Model**   | 44       |           0.79 | \n",
    "| **Polynomial Model (Degree 2)**     |34     |            0.87 | \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> We can see that RMSE has decreased and R²-score has increased as compared to the linear line.\n",
    "\n",
    "Let's have a look at the residual plots... Do you think the points are randomly distributed or can you still se a pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, that even though residuals got a bit smaller (linear regression had residuals from -100 to 60), we can still see some obvious patterns. \n",
    "Adding even higher degrees of polynomials might help to capture this pattern..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model with 3-degree polynomials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add more flexibility. We can also add another degree and fit a cubic curve (degree=3) to the dataset. In the next cell you can try to do it on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will add the second degree polynomial features\n",
    "polynomial_features = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_poly_train = polynomial_features.fit_transform(X_train)\n",
    "x_poly_test = polynomial_features.transform(X_test)\n",
    "\n",
    "model_3_poly = LinearRegression()\n",
    "model_3_poly.fit(x_poly_train, y_train)\n",
    "y_pred_test = model_3_poly.predict(x_poly_test)\n",
    "y_pred_train = model_3_poly.predict(x_poly_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polynomial Regression Model (degree 3):\")\n",
    "print(\"---\"*10)\n",
    "calculate_metrics(y_train,y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_poly_3_reg = model_3_poly.intercept_ + model_3_poly.coef_[0][0] * x_plot + model_3_poly.coef_[0][1] * x_plot**2 + model_3_poly.coef_[0][2] * x_plot**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(X_train, y_train, s=10,c=\"#193251\",alpha=0.7, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",alpha=0.7, label= '\"True\" Relationship')\n",
    "plt.plot(x_plot,line_lin_reg,c=\"#FFBA08\",alpha=0.5,label= 'Linear model')\n",
    "plt.plot(x_plot,line_poly_2_reg,c=\"#FFBA08\", alpha=0.5,label= 'Degree 2 model')\n",
    "plt.plot(x_plot,line_poly_3_reg,c=\"#FFBA08\",marker = \"o\",ms=1, alpha=0.8,label= 'Degree 3 model')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see that the linear model with cubic features is almost identical with the data generating process.\n",
    "The RMSE for the training and test data are also low compared to the other models.\n",
    "\n",
    "\n",
    "| Metric   | RMSE  | R²  | \n",
    "|---|---|---|\n",
    "| **Linear Model**   | 44       |           0.79 | \n",
    "| **Polynomial Model (Degree 2)**     |34     |            0.87 | \n",
    "| **Polynomial Model (Degree 3)**     |19     |            0.96 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the residual plots, we see that the range of residuals is way smaller and the is no general pattern visible. This hints that the model  is capturing all relationship between features and target variable that is present in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model with 8-degree polynomials\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an even more flexible model. This model should also be able to catch the patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will add the second degree polynomial features\n",
    "polynomial_features = PolynomialFeatures(degree=8, include_bias=False)\n",
    "x_poly_train = polynomial_features.fit_transform(X_train)\n",
    "x_poly_test = polynomial_features.transform(X_test)\n",
    "\n",
    "model_8_poly = LinearRegression()\n",
    "model_8_poly.fit(x_poly_train, y_train)\n",
    "y_pred_test = model_8_poly.predict(x_poly_test)\n",
    "y_pred_train = model_8_poly.predict(x_poly_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polynomial Regression Model (degree 8):\")\n",
    "print(\"---\"*10)\n",
    "calculate_metrics(y_train,y_pred_train, y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is a typical sign for overfitting! The RMSE on the training data is pretty low: with 14 even lower than the previous model (15). But on the other side increased the RMSE on the unseen test data enormously! \n",
    "Overfitting happens when the model unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented the underlying relationship. Using this model won't give good results as the included irrelevant information (noise) in the model won't help to predict y for unseen data.\n",
    "\n",
    "| Metric   | RMSE  | R²  | \n",
    "|---|---|---|\n",
    "| **Linear Model**   | 44       |           0.79 | \n",
    "| **Polynomial Model (Degree 2)**     |34     |            0.87 | \n",
    "| **Polynomial Model (Degree 3)**     |19     |            0.96 | \n",
    "| **Polynomial Model (Degree 8)**     |33     |            0.88 | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_poly_8_reg = model_8_poly.intercept_ + model_8_poly.coef_[0][0] * x_plot + model_8_poly.coef_[0][1] * x_plot**2 + model_8_poly.coef_[0][2] * x_plot**3 + model_8_poly.coef_[0][3] * x_plot**4 + model_8_poly.coef_[0][4] * x_plot**5 + model_8_poly.coef_[0][5] * x_plot**6 + model_8_poly.coef_[0][6] * x_plot**7 + model_8_poly.coef_[0][7] * x_plot**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(X_train, y_train, s=10,c=\"#193251\",alpha=0.7, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",alpha=0.7, label= '\"True\" Relationship')\n",
    "#plt.plot(x_plot,line_lin_reg,c=\"#FFBA08\",alpha=0.5,label= 'Linear model')\n",
    "#plt.plot(x_plot,line_poly_2_reg,c=\"#FFBA08\", alpha=0.5,label= 'Degree 2 model')\n",
    "#plt.plot(x_plot,line_poly_3_reg,c=\"#FFBA08\", alpha=0.5,label= 'Degree 3 model')\n",
    "plt.plot(x_plot,line_poly_8_reg,c=\"#FFBA08\",marker = \"o\",ms=1, alpha=0.8,label= 'Degree 8 model')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is way more flexible than the other models we have used so far, it wiggles more to fit the training data. The model with polynomial features up to 8 degree is further away from out data generating process as the previous model.\n",
    "We see that this line might fit the training data very well, but it's not fitting so well anymore for unknown test data.\n",
    "\n",
    "Let's just look at the training data for our 4 models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(X_train, y_train, s=10,c=\"#193251\",alpha=0.7, label=\"Train data\")\n",
    "#plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",alpha=0.7, label= '\"True\" Relationship')\n",
    "plt.plot(x_plot,line_lin_reg,c=\"#FFBA08\",alpha=0.5,label= 'Linear model')\n",
    "#plt.plot(x_plot,line_poly_2_reg,c=\"#FFBA08\", alpha=0.5,label= 'Degree 2 model')\n",
    "plt.plot(x_plot,line_poly_3_reg,c=\"#FFBA08\", alpha=0.5,label= 'Degree 3 model')\n",
    "plt.plot(x_plot,line_poly_8_reg,c=\"#FFBA08\",marker = \"o\",ms=1, alpha=0.8,label= 'Degree 8 model')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that with increasing flexibility, the models fit the training data better. While the linear model with polynomial of oder 1 clearly fails to capture the variation in the data, the linear model with polynomials up to order 8 fit the training data very well but they might also have a hard time on novel instances (= **generalization**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual plot for this model looks not totally bad for most parts. But there were two instances where our model predicted extremely bad. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"High variance\" model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ok... let's have one more look at what \"high-variance\" models mean.\n",
    "\n",
    "Variance is the amount that the estimate of the target will change if different training data was used. How sensitive is the algorithm to the specific training data used.\n",
    "\n",
    "To check what this exactly means in our case here, we will train now 3 nonflexible models (polynomials degree of 1) and 3 highly flexible models (polynomials degree of 8) on different training data!\n",
    "And see how much those models vary from one another.\n",
    "\n",
    "So first step is to create 2 other train test splits, to get 2 more training sets with which we can fit our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split2, X_test_split2, y_train_split2, y_test_split2 = train_test_split(x, y, test_size=0.8, random_state=91)\n",
    "X_train_split3, X_test_split3, y_train_split3, y_test_split3 = train_test_split(x, y, test_size=0.8, random_state=222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will first use these new data to train 3 linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Model 2\n",
    "lin_reg_2 = LinearRegression()\n",
    "lin_reg_2.fit(X_train_split2, y_train_split2)\n",
    "\n",
    "# Model 3\n",
    "lin_reg_3 = LinearRegression()\n",
    "lin_reg_3.fit(X_train_split3, y_train_split3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot those three models to see how much they differ from one another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot_true = 10 * x_plot - 2 * (x_plot ** 2) + 0.5 * (x_plot ** 3)\n",
    "line_lin_reg = lin_reg.intercept_ + lin_reg.coef_[0]*x_plot\n",
    "line_lin_reg_2 = lin_reg_2.intercept_ + lin_reg_2.coef_[0]*x_plot\n",
    "line_lin_reg_3 = lin_reg_3.intercept_ + lin_reg_3.coef_[0]*x_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(x, y, s=10,c=\"#193251\",alpha=0.7, label=\"All Data\")\n",
    "#plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",label= '\"True\" Relationship')\n",
    "plt.plot(x_plot,line_lin_reg,c=\"#FFBA08\",label= 'Linear model 1')\n",
    "plt.plot(x_plot,line_lin_reg_2,c=\"#AAAE90\",label= 'Linear model 2')\n",
    "plt.plot(x_plot,line_lin_reg_3,c=\"#FFFA54\",label= 'Linear model 3')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Comparison of linear models using different training data\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three linear models aren't identical. But the intercepts and the scopes are pretty similar. \n",
    "\n",
    "These models aren't very sensitive to changes in training data.\n",
    "\n",
    "Let's see how the more flexible models perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=8, include_bias=False)\n",
    "x_poly_train = polynomial_features.fit_transform(X_train)\n",
    "x_poly_test = polynomial_features.transform(X_test)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=8, include_bias=False)\n",
    "x_poly_train_2 = polynomial_features.fit_transform(X_train_split2)\n",
    "x_poly_test_2 = polynomial_features.transform(X_test_split2)\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=8, include_bias=False)\n",
    "x_poly_train_3 = polynomial_features.fit_transform(X_train_split3)\n",
    "x_poly_test_3 = polynomial_features.transform(X_test_split3)\n",
    "\n",
    "\n",
    "# Flexible model 1\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(x_poly_train, y_train)\n",
    "\n",
    "# Flexible model 2\n",
    "poly_reg_2 = LinearRegression()\n",
    "poly_reg_2.fit(x_poly_train_2, y_train_split2)\n",
    "\n",
    "# Flexible model 3\n",
    "poly_reg_3 = LinearRegression()\n",
    "poly_reg_3.fit(x_poly_train_3, y_train_split3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_plot_true = 10 * x_plot - 2 * (x_plot ** 2) + 0.5 * (x_plot ** 3)\n",
    "line_poly_1 = poly_reg.intercept_ + poly_reg.coef_[0][0] * x_plot + poly_reg.coef_[0][1] * x_plot**2 + poly_reg.coef_[0][2] * x_plot**3 + poly_reg.coef_[0][3] * x_plot**4 + poly_reg.coef_[0][4] * x_plot**5 + poly_reg.coef_[0][5] * x_plot**6 + poly_reg.coef_[0][6] * x_plot**7 + poly_reg.coef_[0][7] * x_plot**8\n",
    "line_poly_2 = poly_reg_2.intercept_ + poly_reg_2.coef_[0][0] * x_plot + poly_reg_2.coef_[0][1] * x_plot**2 + poly_reg_2.coef_[0][2] * x_plot**3 + poly_reg_2.coef_[0][3] * x_plot**4 + poly_reg_2.coef_[0][4] * x_plot**5 + poly_reg_2.coef_[0][5] * x_plot**6 + poly_reg_2.coef_[0][6] * x_plot**7 + poly_reg_2.coef_[0][7] * x_plot**8\n",
    "line_poly_3 = poly_reg_3.intercept_ + poly_reg_3.coef_[0][0] * x_plot + poly_reg_3.coef_[0][1] * x_plot**2 + poly_reg_3.coef_[0][2] * x_plot**3 + poly_reg_3.coef_[0][3] * x_plot**4 + poly_reg_3.coef_[0][4] * x_plot**5 + poly_reg_3.coef_[0][5] * x_plot**6 + poly_reg_3.coef_[0][6] * x_plot**7 + poly_reg_3.coef_[0][7] * x_plot**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \n",
    "plt.scatter(x, y, s=10,c=\"#193251\",alpha=0.7, label=\"All data\")\n",
    "#plt.scatter(X_test, y_test, s=10,c=\"#FF5A36\",alpha=0.7, label=\"Test data\")\n",
    "plt.plot(x_plot,y_plot_true,c=\"#5E5E5E\",label= '\"True\" Relationship')\n",
    "plt.plot(x_plot,line_poly_1,c=\"#FFBA08\",label= 'Polynomial model 1')\n",
    "plt.plot(x_plot,line_poly_2,c=\"b\",label= 'Polynomial model 2')\n",
    "plt.plot(x_plot,line_poly_3,c= \"r\",label= 'Polynomial model 3')\n",
    "plt.legend()\n",
    "plt.xlabel(\"feature x\")\n",
    "plt.ylabel(\"target variable y\")\n",
    "plt.title(\"Comparison of highly flexible models using different training data\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see, that even though these flexible models fit the training data very well, they don't generalize well; they won't be able to predict well on unseen data. \n",
    "These flexible models change a lot depending on different training data: they are high-variance models! \n",
    "\n",
    "In order to understand how these curves came about, you can plot the training data for the individual curves separately from the test data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook you have seen the typical data science workflow, starting with splitting the data into train and test set, trying out different models and evaluate their performance according to a specified metric and an error analysis.\n",
    "\n",
    "\n",
    "- You learnt to create more flexible linear models by using polynomial features.\n",
    "\n",
    "\n",
    "- You know that the bias-variance tradeoff is a basic concept in machine learning which will play a major role when applying any machine learning algorithm. \n",
    "\n",
    "\n",
    "- You know that Bias is the simplifying assumptions made by the model to make the target function easier to approximate.\n",
    "\n",
    "\n",
    "- Variance is the amount that the estimate of the target function will change given different training data.\n",
    "\n",
    "\n",
    "- The bias-variance Trade-off describes the fact, that it might be wise to use a biased estimator, so long as it reduces the variance, assuming our goal is to minimize the error function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your understanding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the RMSE (for train and test data) of the last 6 models (3 linear regression and 3 polynomial regression (order 8)). What do you expect?\n",
    "\n",
    "2. Train (8) polynomial models with different degrees (from 1-8 polynomial degree) of features. Plot the train RMSE and test RMSE of the models over the model flexibilities (polynomial degree 1-8) according to the very first plot (from wikipedia) in this notebook. What trend do you see in the train RMSE and test RMSE and what explains these trends?\n",
    "\n",
    "3. After trying out these different models, which model would you choose and why? \n",
    "\n",
    "4. What can you do to prevent overfitting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
